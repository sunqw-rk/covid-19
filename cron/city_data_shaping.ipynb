{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d83098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  Tokyo  already exists\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import datetime as dt\n",
    "import os\n",
    "from class_state_vec import state_vector\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "country_name  =  \"Tokyo\"#sys.argv[1] #\n",
    "sv = state_vector()\n",
    "sv.setCountry(country_name)\n",
    "sv.setPopulation(int(13951636))  #sys.argv[2]))#Kyo2563192)# #125360000)#)#J125360000)#9242724)#8815376)#5441276) #125754000)\n",
    "\n",
    "try:\n",
    "    os.mkdir(country_name)\n",
    "    print(\"Directory \" , country_name ,  \" Created \") \n",
    "    first_time = True\n",
    "    sv.setCondition(1) #full outbreak analysis\n",
    "except FileExistsError:\n",
    "    first_time = False\n",
    "    sv.setCondition(2) #daily analysis\n",
    "    print(\"Directory \" , country_name ,  \" already exists\")\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "\n",
    "#read csv\n",
    "#Tsuzu-san made update on 17th Sep 2021\n",
    "if country_name != 'Japan':\n",
    "    #url = \"https://toyokeizai.net/sp/visual/tko/covid19/csv/prefectures.csv\"\n",
    "    #csv_data = requests.get(url).content\n",
    "    path = \"toyokeizai/prefectures.csv\"\n",
    "    df = pd.read_csv(path, sep=\",\")\n",
    "    df.head()\n",
    "\n",
    "    #drop unnecessary columns and shape date \n",
    "    df_tokyo = df[df['prefectureNameE'] == country_name].reset_index() \n",
    "    df_tokyo['date'] = pd.to_datetime({'year': df_tokyo['year'], 'month': df_tokyo['month'], 'day': df_tokyo['date']})\n",
    "    df_tokyo.drop(columns = ['index', 'year', 'month', 'prefectureNameJ', 'prefectureNameE'], inplace=True) \n",
    "    df_tokyo.head()\n",
    "\n",
    "    #make our data\n",
    "    df_data = pd.DataFrame({'date': df_tokyo['date'], \n",
    "                            'daily_confirmed': 0, \n",
    "                            'acc_confirmed': df_tokyo['testedPositive'], \n",
    "                            'daily_hos_add': 0, \n",
    "                            'acc_death': df_tokyo['deaths'], \n",
    "                            'acc_recovered': df_tokyo['discharged'], \n",
    "                            'serious': df_tokyo['serious'], \n",
    "                            'daily_death': 0, \n",
    "                            'effR': df_tokyo['effectiveReproductionNumber']})\n",
    "    #Qiwen added the following two lines to replace the first several days NaN records by 0\n",
    "    df_data['acc_death'].fillna(0, inplace = True)\n",
    "    df_data['acc_recovered'].fillna(0, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_data['daily_confirmed'] = df_data['acc_confirmed'].diff()\n",
    "    df_data['daily_hos_add'] = df_data['acc_confirmed'] - df_data['acc_death'] - df_data['acc_recovered']\n",
    "    #Qiwen 2021-11-02 missing data\n",
    "    #df_data['daily_hos_add'] = df_data['acc_confirmed'] - df_data['acc_death'] - (df_data['acc_recovered']+4512-447)\n",
    "    \n",
    "    df_data['daily_death'] = df_data['acc_death'].diff()\n",
    "    \n",
    "    #Qiwen modify 2021-11-05 Tokyo should always modifies the recovery data, no need to modify data for other cities\n",
    "    if country_name == 'Tokyo':\n",
    "        df_data['acc_recovered'] = df_data['acc_recovered'] -4512 + 447 #we keep the incorrect record to avoid the big jump, since Tokyo does not provide the exact release time.\n",
    "\n",
    "        \n",
    "    df_data.head()\n",
    "    \n",
    "    #fill nan\n",
    "    df_data.fillna(0, inplace=True)\n",
    "    df_data.head()\n",
    "    \n",
    "    #cut data from before 2020/3/6\n",
    "    base_date = dt.datetime(2020, 3, 6)\n",
    "    df_data = df_data[base_date <= df_data['date']].reset_index()\n",
    "    df_data.drop(columns = 'index', inplace=True)\n",
    "\n",
    "    #all value are int except date and effR\n",
    "    for i in df_data.columns.values:\n",
    "        if (i == 'date') or (i == 'effR') :\n",
    "            pass\n",
    "        else:\n",
    "            df_data[i] = df_data[i].astype(int)\n",
    "    df_data_with_makeup = df_data\n",
    "else:\n",
    "    name_lst = [\"confirmed_cases_cumulative_daily\", \"requiring_inpatient_care_etc_daily\", \"deaths_cumulative_daily\",\n",
    "                \"severe_cases_daily\",\"effective_reproduction_number\"]     \n",
    "    url_lst = []\n",
    "    for name in name_lst:\n",
    "        if name != \"effective_reproduction_number\":\n",
    "            #url = \"https://toyokeizai.net/sp/visual/tko/covid19/csv/{}.csv\".format(name)\n",
    "            #url = \"https://covid19.mhlw.go.jp/public/opendata/{}.csv\".format(name)\n",
    "            url = \"mhlw/{}.csv\".format(name)\n",
    "        else:\n",
    "            #url = \"https://toyokeizai.net/sp/visual/tko/covid19/csv/effective_reproduction_number.csv\"\n",
    "            url = \"toyokeizai/effective_reproduction_number.csv\"\n",
    "        url_lst.append(url)\n",
    "\n",
    "    df_lst = []\n",
    "    count =1\n",
    "    base_date = dt.datetime(2020, 5, 9)\n",
    "    for url in url_lst:\n",
    "        #csv_data = requests.get(url).content\n",
    "        #df = pd.read_csv(io.BytesIO(csv_data), sep=\",\")\n",
    "        df = pd.read_csv(url, sep=\",\")\n",
    "        '''\n",
    "        if count<len(name_lst):\n",
    "            for q in range(len(df)):\n",
    "                df['Date'][q] = pd.to_datetime(df['Date'][q])\n",
    "        df= df[base_date <= df['Date']].reset_index()\n",
    "        df.drop(columns = 'index', inplace=True)\n",
    "       '''\n",
    "        if count == len(name_lst):\n",
    "            df['日付'] = pd.to_datetime(df['日付'])\n",
    "            df_ern = df[base_date<=df['日付']].reset_index()\n",
    "            df_ern.drop(columns = 'index', inplace=True)\n",
    "            break\n",
    "        \n",
    "        df_lst.append(df)\n",
    "        count = count+1\n",
    "\n",
    "    for i in range(len(df_lst)):\n",
    "        #df_lst[i]['日付'] = pd.to_datetime(df_lst[i]['日付'])\n",
    "        df_lst[i]['Date'] = pd.to_datetime(df_lst[i]['Date'])\n",
    "        \n",
    "    #2021-12-07 Qiwen   mhlw change the data format, qiwen modified the code correspondingly\n",
    "    if country_name == 'Japan':\n",
    "        df = pd.concat([df_lst[0]['Date'], df_lst[0]['ALL'], \n",
    "                        df_lst[1]['(ALL) Discharged from hospital or released from treatment'],\n",
    "                        df_lst[2]['ALL'],\n",
    "                        df_lst[3]['ALL']],axis = 1)\n",
    "    else:\n",
    "        df = pd.concat([df_lst[0]['Date'], df_lst[0][country_name], \n",
    "                        df_lst[1]['(%s) Discharged from hospital or released from treatment'%(country_name)],\n",
    "                        df_lst[2][country_name],\n",
    "                        df_lst[3][country_name]],axis = 1)\n",
    "    df_data = pd.DataFrame({'date':np.zeros(len(df.iloc[:,[0]])), \n",
    "                            'daily_confirmed':np.zeros(len(df.iloc[:,[0]])),\n",
    "                            'acc_confirmed':np.zeros(len(df.iloc[:,[0]])), \n",
    "                            'daily_hos_add':np.zeros(len(df.iloc[:,[0]])), \n",
    "                            'acc_death':np.zeros(len(df.iloc[:,[0]])), \n",
    "                            'acc_recovered':np.zeros(len(df.iloc[:,[0]])),\n",
    "                            'serious':np.zeros(len(df.iloc[:,[0]])),\n",
    "                            'daily_death':np.zeros(len(df.iloc[:,[0]])), \n",
    "                            'effR':np.zeros(len(df.iloc[:,[0]]))})\n",
    "    df_data.iloc[:,[0,2,4,5,6]]=  df.iloc[:, [0,1,3,2,4]].copy()         \n",
    "    #--#\n",
    "    \n",
    "    df_data.head()\n",
    "    \n",
    "    df_data['daily_confirmed'] = df_data['acc_confirmed'].diff()\n",
    "    df_data['daily_hos_add'] = df_data['acc_confirmed'] - df_data['acc_death'] - df_data['acc_recovered']\n",
    "    df_data['daily_death'] = df_data['acc_death'].diff()\n",
    "    \n",
    "    #20211108 Qiwen add for after 20211028 Japan\n",
    "    df_data['acc_recovered'] = df_data['acc_recovered'] - 4512 + 447\n",
    "    \n",
    "    df_data.fillna(0, inplace=True)\n",
    "\n",
    "    for i in df_data.columns.values:\n",
    "        if (i == 'date') or (i == 'effR') :\n",
    "            pass\n",
    "        else:\n",
    "            df_data[i] = df_data[i].astype(int)\n",
    "\n",
    "    #df_data = df_data[50:].reset_index()\n",
    "    #df_data.drop(columns = 'index', inplace=True)\n",
    "    df_data.head()\n",
    "    \n",
    "    dd = df_data[0:64]\n",
    "    fra = [df_data[0:64],df_data]\n",
    "    results = pd.concat(fra,ignore_index = True, join='inner')\n",
    "    df_data_with_makeup = results.reset_index()\n",
    "    df_data_with_makeup.drop(columns = 'index', inplace=True)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "if first_time:\n",
    "    df_data.to_csv('%s/data_full_all.csv'%(country_name))\n",
    "    df_data.to_csv('%s/data_full_all_history.csv'%(country_name))\n",
    "    #df_data[['date', 'daily_hos_add', 'acc_recovered','acc_death' ]].to_csv('%s/data_full.csv'%(country_name), header = None)\n",
    "else:\n",
    "    old_data = pd.read_csv ('%s/data_full_all_history.csv'%(country_name))\n",
    "    #20220128\n",
    "    #df_2days=df_data_with_makeup[len(old_data)-1:len(old_data)+1].reset_index()\n",
    "    df_2days=df_data_with_makeup[len(old_data)-1:].reset_index()\n",
    "    ##\n",
    "    if len(df_2days)<2:\n",
    "        print('no new data')\n",
    "    else:\n",
    "        df_2days.drop(columns = 'index', inplace=True)\n",
    "        df_2days.to_csv('%s/data_full_all.csv'%(country_name), index=False)\n",
    "        #Qiwen 2021-11-2 keep the old observations for the time being\n",
    "        frames = [old_data, df_2days[-len(df_2days)+1:]]\n",
    "        result = pd.concat(frames,ignore_index = True, join='inner')\n",
    "        result['date']=pd.to_datetime(result.date)\n",
    "        result.to_csv('%s/data_full_all_history.csv'%(country_name))\n",
    "        #df_data[:len(old_data)+1].to_csv('%s/data_full_all_history.csv'%(country_name))\n",
    "        #df_data[['date', 'daily_hos_add', 'acc_recovered','acc_death' ]][0:len(old_data)+1].to_csv('%s/data_full.csv'%(country_name), header = None)\n",
    "    \n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "df_data[\"date\"] = pd.to_datetime(df_data[\"date\"]).dt.strftime(\"%Y%m%d\")\n",
    "df_data_with_makeup[\"date\"] = pd.to_datetime(df_data_with_makeup[\"date\"]).dt.strftime(\"%Y%m%d\")\n",
    "if first_time:\n",
    "    df_data_today = df_data_with_makeup\n",
    "else:\n",
    "    df_data_today = df_data_with_makeup\n",
    "bb = (df_data_today[\"date\"][-len(df_2days):])\n",
    "bb = np.array(bb)\n",
    "sv.setDate(bb)\n",
    "\n",
    "sv.save('x_nature.pkl')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c4e8e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20211231', '20220101', '20220102', '20220103', '20220104',\n",
       "       '20220105', '20220106', '20220107', '20220108', '20220109',\n",
       "       '20220110', '20220111', '20220112', '20220113', '20220114',\n",
       "       '20220115', '20220116', '20220117', '20220118', '20220119',\n",
       "       '20220120', '20220121', '20220122', '20220123', '20220124',\n",
       "       '20220125', '20220126', '20220127', '20220128', '20220129',\n",
       "       '20220130', '20220131', '20220201', '20220202', '20220203',\n",
       "       '20220204', '20220205', '20220206', '20220207', '20220208',\n",
       "       '20220209', '20220210', '20220211', '20220212', '20220213',\n",
       "       '20220214', '20220215', '20220216', '20220217', '20220218',\n",
       "       '20220219', '20220220', '20220221', '20220222', '20220223',\n",
       "       '20220224', '20220225', '20220226', '20220227', '20220228'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d07c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
